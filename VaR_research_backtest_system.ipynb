{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T07:43:49.167906Z",
     "start_time": "2019-09-03T07:43:49.156416Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#import\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql\n",
    "import operator\n",
    "import os\n",
    "import math\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from datetime import *\n",
    "import scipy.io as sio\n",
    "import h5py\n",
    "import copy\n",
    "import shutil\n",
    "import rarfile\n",
    "import random\n",
    "#from general_function import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.font_manager import FontProperties\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False  #用来正常显示负号\n",
    "sns.set(style=\"whitegrid\")\n",
    "#myfont = FontProperties(fname=r'C:\\Windows\\Fonts\\simhei.ttf', size=10)\n",
    "#sns.set(font=myfont.get_name(),style=\"whitegrid\",palette=\"muted\",color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T09:08:09.166583Z",
     "start_time": "2019-04-18T09:07:36.266522Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data\n",
    "os.chdir(r'C:\\Users\\bj\\Desktop\\刘泽丹\\Code')\n",
    "dateparse = lambda dates: pd.datetime.strptime(str(date.fromordinal(int(dates)-366)).replace('-',''), '%Y%m%d')\n",
    "zzIcData = pd.read_csv('zz500_from_db.csv',index_col = 0, date_parser=dateparse)\n",
    "dateparse = lambda dates: pd.datetime.strptime(dates, '%Y%m%d')\n",
    "open_p = pd.read_csv('open.csv',index_col = 0, date_parser=dateparse)\n",
    "close_p = pd.read_csv('close.csv',index_col = 0, date_parser=dateparse)\n",
    "amt = pd.read_csv('amt.csv',index_col = 0, date_parser=dateparse)\n",
    "adjfactor = pd.read_csv('adjfactor.csv',index_col = 0, date_parser=dateparse)\n",
    "open_p.columns = [int(i) for i in open_p.columns]\n",
    "close_p.columns = [int(i) for i in close_p.columns]\n",
    "amt.columns = [int(i) for i in amt.columns]\n",
    "adjfactor.columns = [int(i) for i in adjfactor.columns]\n",
    "adjopen = adjfactor * open_p\n",
    "adjclose = adjfactor * close_p\n",
    "adjpreclose = adjclose.shift(1)\n",
    "close_to_close = np.log(adjclose/adjpreclose)\n",
    "open_to_close = np.log(adjclose/adjopen)\n",
    "preclose_to_open = np.log(adjopen/adjpreclose)\n",
    "# close_to_close = adjclose/adjpreclose - 1\n",
    "# open_to_close = adjclose/adjopen - 1\n",
    "# preclose_to_open = adjopen/adjpreclose - 1\n",
    "amt = amt.replace(0,np.nan)\n",
    "amt = amt.T.dropna(how='all').T\n",
    "def get_rid_of_newly_ipos(df):\n",
    "    return df.iloc[df.index.tolist().index(df[pd.notnull(df)].index[0])+42:]\n",
    "amt = amt.apply(get_rid_of_newly_ipos)\n",
    "close_to_close = close_to_close[pd.notnull(amt)].dropna(how='all')\n",
    "open_to_close = open_to_close[pd.notnull(amt)].dropna(how='all')\n",
    "preclose_to_open = preclose_to_open[pd.notnull(amt)].dropna(how='all')\n",
    "zz500 = (zzIcData['5']/(zzIcData['5'].shift(1).fillna(1000)) - 1)\n",
    "zz500.name = 'zz500'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T09:08:09.185532Z",
     "start_time": "2019-04-18T09:08:09.169579Z"
    },
    "code_folding": [
     0,
     13,
     17,
     23,
     40,
     57
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def future_value_df_generator(var_list,ret_list,tw = 100):\n",
    "    var_list.name = 'VaR'\n",
    "    var_to_test_data = pd.DataFrame(var_list.dropna())\n",
    "    for i in range(tw):\n",
    "        var_to_test_data[i] = np.nan\n",
    "    for i in var_to_test_data.index:\n",
    "        test_data = ret_list.iloc[ret_list.index.tolist().index(i)+1:].iloc[:tw].reset_index(drop=True)\n",
    "        if len(test_data) != tw:\n",
    "            break\n",
    "        for j in range(tw):\n",
    "            var_to_test_data.loc[i,j] = test_data.loc[j]\n",
    "    var_to_test_data.dropna(inplace=True)\n",
    "    return var_to_test_data\n",
    "def count_exception(df, tw):\n",
    "    VaR = df.loc['VaR']\n",
    "    test_data = df.loc[range(tw)]\n",
    "    return len(test_data[test_data<VaR])\n",
    "def kupiec_backtest(df, cl, tw):\n",
    "    cl_a = 1 - cl\n",
    "    LRuc_left = -2*np.log((cl**(tw - df.loc['n_exception'])) * (cl_a**df.loc['n_exception']))\n",
    "    LRuc_right = 2*np.log(((1 - df.loc['n_exception']/tw)**(tw - df.loc['n_exception']) *\\\n",
    "                           ((df.loc['n_exception']/tw)**df.loc['n_exception'])))\n",
    "    return LRuc_left + LRuc_right\n",
    "def christoffersen_test(df, tw):\n",
    "    VaR = df.loc['VaR']\n",
    "    n_exception = df.loc['n_exception']\n",
    "    if n_exception == 0 or (n_exception == 1 and df.loc[tw-1] < VaR):\n",
    "        return 0\n",
    "    test_data = df.loc[range(tw)].tolist()\n",
    "    n_00 = len([value for index,value in enumerate(test_data[:-1]) if value > VaR and test_data[index+1] > VaR])\n",
    "    n_10 = len([value for index,value in enumerate(test_data[:-1]) if value < VaR and test_data[index+1] > VaR])\n",
    "    n_01 = len([value for index,value in enumerate(test_data[:-1]) if value > VaR and test_data[index+1] < VaR])\n",
    "    n_11 = len([value for index,value in enumerate(test_data[:-1]) if value < VaR and test_data[index+1] < VaR])\n",
    "    pi_0 = n_01 / (n_00 + n_01) if n_00 + n_01 != 0 else 0\n",
    "    pi_1 = n_11 / (n_10 + n_11) if n_10 + n_11 != 0 else 0\n",
    "    pi = (n_01 + n_11) / (n_00 + n_01 + n_10 + n_11)\n",
    "    LRcc_up = ((1 - pi)**(n_00 + n_10)) * (pi**(n_01+n_11))\n",
    "    LRcc_down = ((1 - pi_0)**n_00) * (pi_0**n_01) * ((1 - pi_1)**n_10) * (pi_1**n_11)\n",
    "    LRcc = -2*np.log(LRcc_up / LRcc_down)\n",
    "    return LRcc\n",
    "def exception_test(df, cl):\n",
    "    LRuc = df.loc['LRuc']\n",
    "    LRcc = df.loc['LRcc']\n",
    "    #uc - chi-sqaure 1 df\n",
    "    #cc - chi-square 2 df\n",
    "    LRuc_limit = stats.chi2.ppf(cl, df = 1)\n",
    "    LRcc_limit = stats.chi2.ppf(cl, df = 2)\n",
    "    if LRcc > LRcc_limit:\n",
    "        if LRuc + LRcc > LRuc_limit + LRcc_limit:\n",
    "            return 'reject'\n",
    "        else:\n",
    "            return 'accept'\n",
    "    else:\n",
    "        if LRuc > LRuc_limit:\n",
    "            return 'reject'\n",
    "        else:\n",
    "            return 'accept'\n",
    "def backtest_VaR(ret_data, var_data, window = 100, var_cl = 0.95, test_cl = 0.95):\n",
    "    var_data.name = 'VaR'\n",
    "    var_confidence_level = var_cl\n",
    "    test_confidence_level = test_cl\n",
    "    data =  future_value_df_generator(var_data,ret_data,tw = window)\n",
    "    data['n_exception'] = data.apply(count_exception,args=(window,),axis=1)\n",
    "    data['LRuc'] = data.apply(kupiec_backtest,args=(var_confidence_level,window,),axis=1)\n",
    "    data['LRcc'] = data.apply(christoffersen_test,args=(window,),axis=1)\n",
    "    data['result'] = data.apply(exception_test,args=(test_confidence_level,),axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T09:08:09.299243Z",
     "start_time": "2019-04-18T09:08:09.291248Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def original_VaR(df, cl):\n",
    "    cl_a = 1 - cl\n",
    "    return sorted(df)[int(len(df)*cl_a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T09:08:20.759595Z",
     "start_time": "2019-04-18T09:08:09.301236Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accept    0.894098\n",
       "reject    0.105902\n",
       "Name: result, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtest_VaR(close_to_close[2072].dropna(), \\\n",
    "             close_to_close[2072].dropna().rolling(window = 100).apply(original_VaR,args=(0.99,)), \\\n",
    "             window = 6, var_cl = 0.99, test_cl = 0.95)['result'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T08:06:02.540289Z",
     "start_time": "2019-04-18T08:06:02.532312Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "2148/(2148+131)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats.probplot((full_oc.xs(1,level=0)['open_to_close'].dropna().sort_values().tolist()),dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats.probplot(pd.Series(np.log((full_oc.xs(1,level=0)['open_to_close']/100+1).dropna().sort_values().tolist())),\\\n",
    "    dist=\"lognorm\", sparams = (0.02,), plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats.probplot(full_oc.xs(1,level=0)['open_to_close'].dropna().sort_values().tolist(), dist=\"t\", sparams = (5,), plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Age weighted VaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_window = 100\n",
    "var_confidence_level = 0.95\n",
    "test_confidence_level = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp = full_cc.xs(1,level=0)['close_to_close'].dropna().iloc[-700:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def original_VaR(df, cl):\n",
    "    cl_a = 1 - cl\n",
    "    return sorted(df)[int(len(df)*cl_a)]\n",
    "exp_var = exp.rolling(window = test_window).apply(original_VaR,args=(var_confidence_level,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "backtest = backtest_VaR(exp,exp_var,window = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "backtest['result'].value_counts()/len(backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_lambda = 0.9\n",
    "param_max_length = 100\n",
    "\n",
    "def age_weighted_VaR(df, cl, ld):\n",
    "    weights = [(ld**(i-1))*(1-ld)/(1-ld**len(df)) for i in range(len(df))][::-1]\n",
    "    mid = pd.DataFrame([df,weights]).T.sort_values(0)\n",
    "    mid[2] = mid[1].cumsum()\n",
    "    cl_a = 1 - cl\n",
    "    return mid[0].iloc[next(index for index, value in enumerate(mid[2]) if value > cl_a)]\n",
    "\n",
    "exp_var = exp.rolling(window = param_max_length, min_periods = test_window).apply(age_weighted_VaR,args=(var_confidence_level,param_lambda,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "backtest = backtest_VaR(exp,exp_var,window = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "backtest['result'].value_counts()/len(backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GARCH model to predict volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import  stats\n",
    "import statsmodels.api as sm  # 统计相关的库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import arch  # 条件异方差模型相关的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = full_cc.xs(1,level=0)['close_to_close'].dropna().iloc[-700:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(data.tolist())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = sm.tsa.stattools.adfuller(data)  # ADF检验\n",
    "print(\"p-value:   \",t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,5))\n",
    "ax1=fig.add_subplot(111)\n",
    "fig = sm.graphics.tsa.plot_pacf(data,lags = 20,ax=ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_lags = 11\n",
    "n_test = 100\n",
    "\n",
    "train = data[:-n_test]\n",
    "test = data[-n_test:]\n",
    "am = arch.arch_model(train,mean='AR',lags=n_lags,vol='GARCH') \n",
    "res = am.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ini = res.resid[-n_lags:]\n",
    "a = np.array(res.params[1:-3])\n",
    "w = a[::-1] # 系数\n",
    "for i in range(n_test):\n",
    "    new = test.iloc[i] - (res.params.iloc[0] + w.dot(ini[-n_lags:]))\n",
    "    ini = np.append(ini,new)\n",
    "at_pre = ini[-n_test:]\n",
    "at_pre2 = at_pre**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ini2 = np.array(res.conditional_volatility[-2:])\n",
    "\n",
    "for i in range(n_test):\n",
    "    new = res.params.loc['omega'] + res.params.loc['alpha[1]']*at_pre2[i] + res.params.loc['beta[1]']*ini2[-1]\n",
    "    ini2 = np.append(ini2,new)\n",
    "vol_pre = ini2[-n_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(data.tolist(),label='origin_data')\n",
    "plt.plot(res.conditional_volatility.tolist(),label='conditional_volatility')\n",
    "x=range(len(train),len(train)+n_test)\n",
    "plt.plot(x,vol_pre,'r-',label='predict_volatility')\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg = OLS(data.diff(1).dropna().tolist(),[[1,i] for i in data.shift(1).loc[data.diff(1).dropna().index].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = reg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(data.shift(1).loc[data.diff(1).dropna().index],data.diff(1).dropna().tolist(),'r.')\n",
    "plt.plot(np.arange(-10,10,0.01)*result.params[1],np.arange(-10,10,0.01),'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "am = arch.arch_model(data,mean='AR',lags=5,vol='GARCH',p = 1, q = 1)\n",
    "index = data.index\n",
    "start_loc = 0\n",
    "end_loc = 200\n",
    "forecasts = {}\n",
    "for i in range(len(data)-end_loc):\n",
    "    sub_res = am.fit(first_obs=i, last_obs=i+end_loc, disp='off')\n",
    "    temp = sub_res.forecast(horizon=1,method = 'bootstrap',simulations = 10).variance\n",
    "    fcast = temp.iloc[i+end_loc-1]\n",
    "    forecasts[fcast.name] = fcast\n",
    "forecasted_vol = pd.DataFrame(forecasts).T**(1/2)\n",
    "conditional_vol = res.conditional_volatility.loc[pd.DataFrame(forecasts).T.index]\n",
    "rolling_std = data.rolling(window=100).std().loc[conditional_vol.index]\n",
    "plt.plot(pd.ewma(forecasted_vol['h.1'],span = 1).tolist(),'r-')\n",
    "plt.plot(rolling_std.tolist(),'g-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Volatility-adjusted VaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vaV = pd.concat([data,rolling_std,pd.ewma(forecasted_vol['h.1'],span = 1)],axis=1,join='inner')\n",
    "vaV.columns = ['ret','std','forecasted_vol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',20)\n",
    "pd.set_option('display.max_columns',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vaV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_series = []\n",
    "VaR_series = []\n",
    "param_lambda = 0.94\n",
    "param_max_length = 100\n",
    "VaR = None\n",
    "old_VaR = None\n",
    "for i in range(len(vaV)):\n",
    "    sub_vaV = vaV.iloc[:i+1].copy()\n",
    "    if len(sub_vaV) < test_window:\n",
    "        continue\n",
    "    if len(sub_vaV) <= param_max_length:\n",
    "        pass\n",
    "    else:\n",
    "        sub_vaV = sub_vaV.iloc[-param_max_length:]\n",
    "        \n",
    "    #newest_forecasted_vol = sub_vaV.iloc[-1,2]\n",
    "    newest_forecasted_vol = sub_vaV.iloc[-1,1]\n",
    "    \n",
    "    sub_vaV['adjusted_ret'] = sub_vaV['ret']*((newest_forecasted_vol/sub_vaV['std']))\n",
    "    index_series.append(sub_vaV.index.tolist()[-1])\n",
    "    \n",
    "    bootstrap_factor = 0.6\n",
    "    VaR_sub_list = []\n",
    "    for k in range(10):\n",
    "        selecting = random.sample([j for j in range(len(sub_vaV))],int(len(sub_vaV)*bootstrap_factor))\n",
    "        sub_sub_vaV = sub_vaV.iloc[selecting]\n",
    "        sub_sub_vaV['weights'] = [(param_lambda**(j-1))*(1-param_lambda)/(1-param_lambda**len(sub_sub_vaV)) \\\n",
    "            for j in range(len(sub_sub_vaV))][::-1]\n",
    "        sub_sub_vaV = sub_sub_vaV.sort_values('adjusted_ret')\n",
    "        sub_sub_vaV['weights_cumsum'] = sub_sub_vaV['weights'].cumsum()\n",
    "    \n",
    "        VaR = sub_sub_vaV['adjusted_ret'].iloc[next(index for index, value in enumerate(sub_sub_vaV['weights_cumsum']) \\\n",
    "                                                        if value > (1-var_confidence_level))]\n",
    "        VaR_sub_list.append(VaR)\n",
    "    VaR = np.mean(VaR_sub_list)\n",
    "\n",
    "    #sub_vaV['weights'] = [(param_lambda**(j-1))*(1-param_lambda)/(1-param_lambda**len(sub_vaV)) \\\n",
    "    #    for j in range(len(sub_vaV))][::-1]\n",
    "    #sub_vaV = sub_vaV.sort_values('adjusted_ret')\n",
    "    #sub_vaV['weights_cumsum'] = sub_vaV['weights'].cumsum()\n",
    "    # \n",
    "    #VaR = sub_vaV['adjusted_ret'].iloc[next(index for index, value in enumerate(sub_vaV['weights_cumsum']) \\\n",
    "    #                                                    if value > (1-var_confidence_level))]\n",
    "    \n",
    "    if old_VaR == None:\n",
    "        old_VaR = VaR\n",
    "        VaR_series.append(VaR)\n",
    "        continue\n",
    "    else:\n",
    "        #if VaR < 1.1 * old_VaR:\n",
    "        #    VaR = 1.1 * old_VaR\n",
    "        #elif VaR > 0.99 * old_VaR:\n",
    "        if VaR > 0.99 * old_VaR:\n",
    "            VaR = 0.99 * old_VaR\n",
    "    old_VaR = VaR\n",
    "    VaR_series.append(VaR)\n",
    "exp_var = pd.Series(VaR_series,index=index_series)\n",
    "\n",
    "exp_var = pd.ewma(exp_var,span = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "backtest = backtest_VaR(data, exp_var, window = 50, var_cl = 0.95, test_cl = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "backtest['result'].value_counts()/len(backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(backtest['VaR'].tolist())\n",
    "ax1.set_ylabel('VaR')\n",
    "\n",
    "ax2 = ax1.twinx()  # this is the important function\n",
    "ax2.plot(backtest['n_exception'].tolist(),'r.')\n",
    "ax2.hlines(0.5,0,len(backtest))\n",
    "ax2.hlines(6.5,0,len(backtest))\n",
    "ax2.set_ylabel('n_exception')\n",
    "plt.show()\n",
    "plt.plot((data.loc[backtest.index]/100+1).cumprod().tolist())\n",
    "plt.ylabel('stock')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quasi-CIR (Cox Ingersoll Ross) model for a volatility process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "while(True):\n",
    "    stock = random.choice(full_cc.index.levels[0].tolist())\n",
    "    data = full_cc.xs(stock,level=0)['close_to_close'].dropna()\n",
    "    data = data[(data<10.1) & (data>-10.1)]\n",
    "    if len(data) <= 601:\n",
    "        print('too short')\n",
    "        continue\n",
    "    else:\n",
    "        print(stock,'with length:',len(data))\n",
    "        amt_data = full_amt.xs(stock,level=0)['amt'].loc[data.index]\n",
    "        std_data = data.rolling(window=100).std().dropna()\n",
    "        #std_data = pd.Series([(i - std_data.min())/(std_data.max() - std_data.min())*10 for i in std_data],index=std_data.index)\n",
    "\n",
    "        fig = plt.figure(figsize=(16,6))\n",
    "\n",
    "        ax1 = fig.add_subplot(211)\n",
    "        ax1.plot(std_data.dropna().tolist())\n",
    "\n",
    "        ax2 = ax1.twinx()  # this is the important function\n",
    "        ax2.bar(range(len(amt_data.loc[std_data.dropna().index])),amt_data.loc[std_data.dropna().index].tolist(),fc='r',alpha = 0.5)\n",
    "\n",
    "        ax3 = fig.add_subplot(212)\n",
    "        ax3.plot(data.loc[std_data.dropna().index].tolist())\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        corr_calc_data_std = (std_data.dropna()/std_data.dropna().shift(50) - 1).dropna()\n",
    "        corr_calc_data_amt = (amt_data.loc[std_data.dropna().index]/amt_data.loc[std_data.dropna().index].shift(1) - 1).dropna()\n",
    "        corr_calc_data_amt = corr_calc_data_amt.loc[corr_calc_data_std.index].dropna()\n",
    "        corr_calc_data_std = corr_calc_data_std.loc[corr_calc_data_amt.index]\n",
    "        \n",
    "        print(np.corrcoef(corr_calc_data_std,corr_calc_data_amt))\n",
    "        print(stats.kendalltau(std_data.dropna().tolist()[50:],amt_data.loc[std_data.dropna().index].tolist()[:-50]).correlation)\n",
    "        \n",
    "        plt.figure(figsize=(10,3))\n",
    "        plt.plot(data.shift(1).loc[data.diff(1).dropna().index],data.diff(1).dropna().tolist(),'r.')\n",
    "        plt.title('return')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10,3))\n",
    "        plt.plot(std_data.shift(1).loc[std_data.diff(1).dropna().index],std_data.diff(1).dropna().tolist(),'r.')\n",
    "        plt.title('volatility')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10,3))\n",
    "        plt.plot((std_data/std_data.shift(1) - 1).shift(1).loc[(std_data/std_data.shift(1) - 1).diff(1).dropna().index],\\\n",
    "                 (std_data/std_data.shift(1) - 1).diff(1).dropna().tolist(),'r.')\n",
    "        plt.title('delta volatility')\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_std_data = []\n",
    "all_std_diff_data = []\n",
    "for stock in full_cc.index.levels[0].tolist():\n",
    "    data = full_cc.xs(stock,level=0)['close_to_close'].dropna()\n",
    "    data = data[(data<10.1) & (data>-10.1)]\n",
    "    amt_data = full_amt.xs(stock,level=0)['amt'].loc[data.index]\n",
    "    amt_data = amt_data.replace(0,np.nan).dropna()\n",
    "    data = data.loc[amt_data.index]\n",
    "    if len(data) <= 100:\n",
    "        continue\n",
    "    else:\n",
    "        std_data = data.rolling(window=100).std()\n",
    "        x = std_data.shift(1).loc[std_data.diff(1).dropna().index].tolist()\n",
    "        y = std_data.diff(1).dropna().tolist()\n",
    "        all_std_data += x\n",
    "        all_std_diff_data += y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(all_std_data,all_std_diff_data,'r.',alpha=0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "volatility_change_data = pd.concat([pd.Series(all_std_data),pd.Series(all_std_diff_data)],axis=1).sort_values(0,ascending=False)\\\n",
    "    .reset_index(drop=True)\n",
    "volatility_change_data.columns = ['volatility','delta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "volatility_change_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interval_to_data = {}\n",
    "minimum_number_in_each_interval = 1000\n",
    "volatility_data_max = volatility_change_data['volatility'].max()\n",
    "volatility_data_min = volatility_change_data['volatility'].min()\n",
    "current_upper = volatility_data_max\n",
    "while(True):\n",
    "    if current_upper <= volatility_data_min:\n",
    "        break\n",
    "    for interval in [0.001,0.005,0.01,0.05,0.1,0.5,1,1.5,2]:\n",
    "        sub_volatility_change_data = volatility_change_data[(volatility_change_data['volatility'] > current_upper-interval) & \\\n",
    "                                                            (volatility_change_data['volatility'] <= current_upper)]\n",
    "        if len(sub_volatility_change_data) >= minimum_number_in_each_interval:\n",
    "            break\n",
    "    interval_to_data[(current_upper,max(current_upper-interval,volatility_data_min-1e-7))] = sub_volatility_change_data\n",
    "    current_upper -= interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_interval = 100\n",
    "interval_to_percentage_distribution = {}\n",
    "for i in interval_to_data.keys():\n",
    "    sub_data_series = interval_to_data[i]['delta'].sort_values()\n",
    "    percentage_distribution = []\n",
    "    for j in range(n_interval):\n",
    "        if j == n_interval - 1:\n",
    "            percentage_distribution.append(sub_data_series.iloc[-(len(sub_data_series)//n_interval):].mean())\n",
    "        else:\n",
    "            percentage_distribution.append(sub_data_series.iloc[j*(len(sub_data_series)//n_interval):\\\n",
    "                                                               (j+1)*(len(sub_data_series)//n_interval)].mean())\n",
    "    interval_to_percentage_distribution[i] = percentage_distribution\n",
    "interval_to_percentage_distribution_df = pd.DataFrame(interval_to_percentage_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interval_to_percentage_distribution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while(True):\n",
    "    stock = random.choice(full_cc.index.levels[0].tolist())\n",
    "    stock = 603389\n",
    "    data = full_cc.xs(stock,level=0)['close_to_close'].dropna()\n",
    "    if len(data) <= 101:\n",
    "        \n",
    "        print('too short')\n",
    "        continue\n",
    "    else:\n",
    "        cut = random.randint(100,len(data)-200)\n",
    "        print(stock)\n",
    "        print(cut)\n",
    "        std_data = data.rolling(window=100).std().dropna()\n",
    "        \n",
    "        new_std_data = std_data.iloc[-cut:]\n",
    "        old_std_data = std_data.iloc[:-cut]\n",
    "        all_std_data = old_std_data.dropna().copy()\n",
    "        data = data.loc[std_data.index]\n",
    "        all_std_data.index = range(-len(all_std_data),0)\n",
    "        all_std_data = all_std_data.append(new_std_data.reset_index(drop=True))\n",
    "        old_std_data = old_std_data.dropna().tolist()\n",
    "        sigma = np.std(old_std_data)*np.sqrt(252)\n",
    "        m = 0.2\n",
    "        theta_mean = np.mean(old_std_data)\n",
    "        theta_min = max(np.min(old_std_data) - m * (np.max(old_std_data) - np.min(old_std_data)), volatility_data_min)\n",
    "        theta_max = min(np.max(old_std_data) + m * (np.max(old_std_data) - np.min(old_std_data)), volatility_data_max)\n",
    "        k = 2.5\n",
    "        fig = plt.figure(figsize=(18,10))\n",
    "        ax1 = fig.add_subplot(211)\n",
    "        ax1.plot(all_std_data)\n",
    "        ending_vol = []\n",
    "        forecast_days = 50\n",
    "        ax1.hlines(theta_mean,all_std_data.index.min(),forecast_days,'g')\n",
    "        ax1.hlines(theta_max,all_std_data.index.min(),forecast_days,'r')\n",
    "        ax1.hlines(theta_min,all_std_data.index.min(),forecast_days,'r')\n",
    "        for j in range(100):\n",
    "            V_0 = new_std_data.iloc[0]\n",
    "            V_n1 = old_std_data[-1]\n",
    "            V_list = [V_n1,V_0]\n",
    "            dt = 1/252\n",
    "\n",
    "            for i in range(forecast_days):\n",
    "                V_old_old = V_list[-2]\n",
    "                V_old = V_list[-1]\n",
    "\n",
    "                choice = random.randint(0,1)\n",
    "                jump = random.randint(0,99)\n",
    "\n",
    "                m = 0.2\n",
    "                theta_min = max(np.min(old_std_data + V_list) - m * (np.max(old_std_data + V_list)\\\n",
    "                                                                     - np.min(old_std_data + V_list)), volatility_data_min)\n",
    "                theta_max = min(np.max(old_std_data + V_list) + m * (np.max(old_std_data + V_list)\\\n",
    "                                                                     - np.min(old_std_data + V_list)), volatility_data_max)\n",
    "\n",
    "                dist_to_max = (theta_max - max(V_old,theta_min))/(theta_max - theta_min)\n",
    "                dist_to_min = (min(V_old,theta_max) - theta_min)/(theta_max - theta_min)\n",
    "                dist_max_mid = (theta_max - theta_mean)/(theta_max - theta_min)\n",
    "                dist_min_mid = (theta_mean - theta_min)/(theta_max - theta_min)\n",
    "\n",
    "                if choice == 0:\n",
    "                    V_new = V_old +\\\n",
    "                        np.abs(dist_to_max - dist_max_mid) * np.abs(dist_to_min - dist_min_mid) * (theta_mean - V_old) * dt * 4 * k+\\\n",
    "                        (dist_to_max * dist_to_min) * (V_old - V_old_old) * dt * 4 * k +\\\n",
    "                        sigma * (dist_to_max * dist_to_min * 100) * V_old * dt * k * k * dt\n",
    "                else:\n",
    "                    V_new = V_old +\\\n",
    "                        np.abs(dist_to_max - dist_max_mid) * np.abs(dist_to_min - dist_min_mid) * (theta_mean - V_old) * dt * 4 * k +\\\n",
    "                        (dist_to_max * dist_to_min) * (V_old - V_old_old) * dt * 4 * k -\\\n",
    "                        sigma * (dist_to_max * dist_to_min * 100) * V_old * dt * k * k * dt\n",
    "                \n",
    "                scaled_V_old = volatility_data_min + dist_to_min * (volatility_data_max - volatility_data_min)\n",
    "                \n",
    "                if jump == 0:\n",
    "                    jump_value = interval_to_percentage_distribution_df[next(col for col in interval_to_percentage_distribution_df\\\n",
    "                        .columns if col[0]>scaled_V_old)].loc[0]\n",
    "                elif jump == 1:\n",
    "                    jump_value = interval_to_percentage_distribution_df[next(col for col in interval_to_percentage_distribution_df\\\n",
    "                        .columns if col[0]>scaled_V_old)].loc[n_interval-1]\n",
    "                else:\n",
    "                    jump_value = 0\n",
    "\n",
    "                V_new += jump_value\n",
    "\n",
    "                V_list.append(V_new)\n",
    "            ending_vol.append(V_new)\n",
    "            ax1.plot(V_list,alpha=0.1)\n",
    "        ax1.plot(0,V_0,'b*')\n",
    "        ax1.plot(forecast_days+1,np.mean(ending_vol),'r*')\n",
    "        ax1.plot(forecast_days+1,min(np.max(ending_vol),np.mean(ending_vol) + 1 * np.std(ending_vol)),'r*')\n",
    "        ax1.plot(forecast_days+1,max(np.min(ending_vol),np.mean(ending_vol) - 1 * np.std(ending_vol)),'r*')\n",
    "        ax1.plot(forecast_days+1,np.max(ending_vol),'r*')\n",
    "        ax1.plot(forecast_days+1,np.min(ending_vol),'r*')\n",
    "        ax1.plot(forecast_days+1,new_std_data.iloc[forecast_days+1],'g*')\n",
    "        ax2 = fig.add_subplot(614)\n",
    "        ax2.plot((data/100+1).cumprod().tolist())\n",
    "        ax3 = fig.add_subplot(615)\n",
    "        ax3.plot(data.tolist())\n",
    "        ax4 = fig.add_subplot(616)\n",
    "        ax4.plot(all_std_data.diff().tolist())\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.iloc[-100:].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_std_data.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while(True):\n",
    "    stock = random.choice(full_cc.index.levels[0].tolist())\n",
    "    data = full_cc.xs(stock,level=0)['close_to_close'].dropna()\n",
    "    data = zz500\n",
    "    std_data = data.rolling(window=100).std()\n",
    "    if len(data) <= 600:\n",
    "        print('too short')\n",
    "        continue\n",
    "    else:\n",
    "        print(stock,len(data))\n",
    "        band_up = []\n",
    "        band_low = []\n",
    "        band_up_lim = []\n",
    "        band_low_lim = []\n",
    "        band_mid = []\n",
    "        for cut in range(len(data)-500,0,-1):\n",
    "            new_std_data = std_data.iloc[-cut:]\n",
    "            old_std_data = std_data.iloc[:-cut]\n",
    "            all_std_data = old_std_data.dropna().copy()\n",
    "            all_std_data.index = range(-len(all_std_data),0)\n",
    "            all_std_data = all_std_data.append(new_std_data.reset_index(drop=True))\n",
    "            old_std_data = old_std_data.dropna().tolist()\n",
    "            sigma = np.std(old_std_data)*np.sqrt(252)\n",
    "            m = 0.2\n",
    "            theta_mean = np.mean(old_std_data)\n",
    "            theta_min = max(np.min(old_std_data) - m * (np.max(old_std_data) - np.min(old_std_data)), volatility_data_min)\n",
    "            theta_max = min(np.max(old_std_data) + m * (np.max(old_std_data) - np.min(old_std_data)), volatility_data_max)\n",
    "            k = 2.5\n",
    "            ending_vol = []\n",
    "            forecast_days = 50\n",
    "            for j in range(100):\n",
    "                V_0 = new_std_data.iloc[0]\n",
    "                V_n1 = old_std_data[-1]\n",
    "                V_list = [V_n1,V_0]\n",
    "                dt = 1/252\n",
    "                \n",
    "                for i in range(forecast_days):\n",
    "                    V_old_old = V_list[-2]\n",
    "                    V_old = V_list[-1]\n",
    "\n",
    "                    choice = random.randint(0,1)\n",
    "                    jump = random.randint(0,99)\n",
    "\n",
    "                    m = 0.2\n",
    "                    theta_min = max(np.min(old_std_data + V_list) - m * (np.max(old_std_data + V_list)\\\n",
    "                                                                         - np.min(old_std_data + V_list)), volatility_data_min)\n",
    "                    theta_max = min(np.max(old_std_data + V_list) + m * (np.max(old_std_data + V_list)\\\n",
    "                                                                         - np.min(old_std_data + V_list)), volatility_data_max)\n",
    "\n",
    "                    dist_to_max = (theta_max - max(V_old,theta_min))/(theta_max - theta_min)\n",
    "                    dist_to_min = (min(V_old,theta_max) - theta_min)/(theta_max - theta_min)\n",
    "                    dist_max_mid = (theta_max - theta_mean)/(theta_max - theta_min)\n",
    "                    dist_min_mid = (theta_mean - theta_min)/(theta_max - theta_min)\n",
    "\n",
    "                    if choice == 0:\n",
    "                        V_new = V_old +\\\n",
    "                            np.abs(dist_to_max - dist_max_mid) * np.abs(dist_to_min - dist_min_mid) * (theta_mean - V_old) * dt * 4 * k+\\\n",
    "                            (dist_to_max * dist_to_min) * (V_old - V_old_old) * dt * 4 * k +\\\n",
    "                            sigma * (dist_to_max * dist_to_min * 100) * V_old * dt * k * k * dt\n",
    "                    else:\n",
    "                        V_new = V_old +\\\n",
    "                            np.abs(dist_to_max - dist_max_mid) * np.abs(dist_to_min - dist_min_mid) * (theta_mean - V_old) * dt * 4 * k +\\\n",
    "                            (dist_to_max * dist_to_min) * (V_old - V_old_old) * dt * 4 * k -\\\n",
    "                            sigma * (dist_to_max * dist_to_min * 100) * V_old * dt * k * k * dt\n",
    "\n",
    "                    scaled_V_old = volatility_data_min + dist_to_min * (volatility_data_max - volatility_data_min)\n",
    "\n",
    "                    if jump == 0:\n",
    "                        jump_value = interval_to_percentage_distribution_df[next(col for col in interval_to_percentage_distribution_df\\\n",
    "                            .columns if col[0]>scaled_V_old)].loc[0]\n",
    "                    elif jump == 1:\n",
    "                        jump_value = interval_to_percentage_distribution_df[next(col for col in interval_to_percentage_distribution_df\\\n",
    "                            .columns if col[0]>scaled_V_old)].loc[n_interval-1]\n",
    "                    else:\n",
    "                        jump_value = 0\n",
    "\n",
    "                    V_new += jump_value\n",
    "\n",
    "                    V_list.append(V_new)\n",
    "                ending_vol.append(V_new)\n",
    "            band_mid.append(np.mean(ending_vol))\n",
    "            band_up.append(np.max(ending_vol))\n",
    "            band_low.append(np.min(ending_vol))\n",
    "            band_up_lim.append(min(np.max(ending_vol),np.mean(ending_vol) + 1 * np.std(ending_vol)))\n",
    "            band_low_lim.append(max(np.min(ending_vol),np.mean(ending_vol) - 1 * np.std(ending_vol)))\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.plot(std_data.iloc[-(len(data)-(500 + forecast_days)):].tolist())\n",
    "        plt.plot(band_mid,'g',alpha = 0.6)\n",
    "        plt.plot(band_up,'r',alpha = 0.6)\n",
    "        plt.plot(band_low,'r',alpha = 0.6)\n",
    "        plt.plot(band_up_lim,'orange',alpha = 0.6)\n",
    "        plt.plot(band_low_lim,'orange',alpha = 0.6)\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.concat([pd.Series(std_data.iloc[-(len(data)-(500 + forecast_days)):].tolist()),\\\n",
    "        pd.Series(band_mid),\\\n",
    "        pd.Series(band_up),\\\n",
    "        pd.Series(band_low),\\\n",
    "        pd.Series(band_up_lim),\\\n",
    "        pd.Series(band_low_lim)],axis=1)\n",
    "result.columns = ['std','mid','up','low','midup','midlow']\n",
    "result.to_csv('zz500stdforecast.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def std_band_future_value_df_generator(var_mid_list,var_up_list,var_low_list,ret_list,tw = 100):    \n",
    "    var_mid_list.name = 'VaR_mid'\n",
    "    var_up_list.name = 'VaR_up'\n",
    "    var_low_list.name = 'VaR_low'\n",
    "    var_to_test_data = pd.concat([var_mid_list.dropna(),var_up_list.dropna(),var_low_list.dropna()],axis = 1)\n",
    "    \n",
    "    for i in range(tw):\n",
    "        var_to_test_data[i] = np.nan\n",
    "    for i in var_to_test_data.index:\n",
    "        test_data = ret_list.iloc[ret_list.index.tolist().index(i)+1:].iloc[:tw].reset_index(drop=True)\n",
    "        if len(test_data) != tw:\n",
    "            break\n",
    "        for j in range(tw):\n",
    "            var_to_test_data.loc[i,j] = test_data.loc[j]\n",
    "            \n",
    "    var_to_test_data.dropna(inplace=True)\n",
    "    \n",
    "    return var_to_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def std_band_count_exception(df, tw, VaR_type):\n",
    "    VaR = df.loc[VaR_type]\n",
    "    test_data = df.loc[range(tw)]\n",
    "    \n",
    "    return len(test_data[test_data<VaR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def std_band_kupiec_backtest(df, cl, tw, exception_type):\n",
    "    cl_a = 1 - cl\n",
    "    \n",
    "    LRuc_left = -2*np.log((cl**(tw - df.loc[exception_type])) * (cl_a**df.loc[exception_type]))\n",
    "    LRuc_right = 2*np.log(((1 - df.loc[exception_type]/tw)**(tw - df.loc[exception_type]) *\\\n",
    "                           ((df.loc[exception_type]/tw)**df.loc[exception_type])))\n",
    "    \n",
    "    return LRuc_left + LRuc_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def std_band_christoffersen_test(df, tw, VaR_type, exception_type):\n",
    "    VaR = df.loc[VaR_type]\n",
    "    n_exception = df.loc[exception_type]\n",
    "    if n_exception == 0 or (n_exception == 1 and df.loc[tw-1] < VaR):\n",
    "        return 0\n",
    "    \n",
    "    test_data = df.loc[range(tw)].tolist()\n",
    "    \n",
    "    n_00 = len([value for index,value in enumerate(test_data[:-1]) if value > VaR and test_data[index+1] > VaR])\n",
    "    n_10 = len([value for index,value in enumerate(test_data[:-1]) if value < VaR and test_data[index+1] > VaR])\n",
    "    n_01 = len([value for index,value in enumerate(test_data[:-1]) if value > VaR and test_data[index+1] < VaR])\n",
    "    n_11 = len([value for index,value in enumerate(test_data[:-1]) if value < VaR and test_data[index+1] < VaR])\n",
    "    \n",
    "    pi_0 = n_01 / (n_00 + n_01)\n",
    "    pi_1 = n_11 / (n_10 + n_11)\n",
    "    pi = (n_01 + n_11) / (n_00 + n_01 + n_10 + n_11)\n",
    "    \n",
    "    LRcc_up = ((1 - pi)**(n_00 + n_10)) * (pi**(n_01+n_11))\n",
    "    LRcc_down = ((1 - pi_0)**n_00) * (pi_0**n_01) * ((1 - pi_1)**n_10) * (pi_1**n_11)\n",
    "    LRcc = -2*np.log(LRcc_up / LRcc_down)\n",
    "    return LRcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def std_band_exception_test(df, cl, uc_type, cc_type):\n",
    "    LRuc = df.loc[uc_type]\n",
    "    LRcc = df.loc[cc_type]\n",
    "    \n",
    "    #uc - chi-sqaure 1 df\n",
    "    #cc - chi-square 2 df\n",
    "    LRuc_limit = stats.chi2.ppf(cl, df = 1)\n",
    "    LRcc_limit = stats.chi2.ppf(cl, df = 2)\n",
    "    \n",
    "    if LRcc > LRcc_limit:\n",
    "        if LRuc + LRcc > LRuc_limit + LRcc_limit:\n",
    "            return 'reject'\n",
    "        else:\n",
    "            return 'accept'\n",
    "    else:\n",
    "        if LRuc > LRuc_limit:\n",
    "            return 'reject'\n",
    "        else:\n",
    "            return 'accept'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def std_band_backtest_VaR(ret_data, var_mid_data, var_up_data, var_low_data, window = 100, var_cl = 0.95, test_cl = 0.95):\n",
    "    var_mid_data.name = 'VaR_mid'\n",
    "    var_up_data.name = 'VaR_up'\n",
    "    var_low_data.name = 'VaR_low'\n",
    "\n",
    "    var_confidence_level = var_cl\n",
    "    test_confidence_level = test_cl\n",
    "    \n",
    "    data =  std_band_future_value_df_generator(var_mid_data,var_up_data,var_low_data,ret_data,tw = window)\n",
    "    data['n_exception_VaR_mid'] = data.apply(std_band_count_exception,args=(window,'VaR_mid',),axis=1)\n",
    "    data['n_exception_VaR_up'] = data.apply(std_band_count_exception,args=(window,'VaR_up',),axis=1)\n",
    "    data['n_exception_VaR_low'] = data.apply(std_band_count_exception,args=(window,'VaR_low',),axis=1)\n",
    "    data['LRuc_VaR_mid'] = data.apply(std_band_kupiec_backtest,args=(var_confidence_level,window,'n_exception_VaR_mid',),axis=1)\n",
    "    data['LRuc_VaR_up'] = data.apply(std_band_kupiec_backtest,args=(var_confidence_level,window,'n_exception_VaR_up',),axis=1)\n",
    "    data['LRuc_VaR_low'] = data.apply(std_band_kupiec_backtest,args=(var_confidence_level,window,'n_exception_VaR_low',),axis=1)\n",
    "    data['LRcc_VaR_mid'] = data.apply(std_band_christoffersen_test,args=(window,'VaR_mid','n_exception_VaR_mid',),axis=1)\n",
    "    data['LRcc_VaR_up'] = data.apply(std_band_christoffersen_test,args=(window,'VaR_up','n_exception_VaR_up',),axis=1)\n",
    "    data['LRcc_VaR_low'] = data.apply(std_band_christoffersen_test,args=(window,'VaR_low','n_exception_VaR_low',),axis=1)\n",
    "    data['result_VaR_mid'] = data.apply(std_band_exception_test,args=(test_confidence_level,'LRuc_VaR_mid','LRcc_VaR_mid',),axis=1)\n",
    "    data['result_VaR_up'] = data.apply(std_band_exception_test,args=(test_confidence_level,'LRuc_VaR_up','LRcc_VaR_up',),axis=1)\n",
    "    data['result_VaR_low'] = data.apply(std_band_exception_test,args=(test_confidence_level,'LRuc_VaR_low','LRcc_VaR_low',),axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vaV = pd.DataFrame([std_data.iloc[-(len(data)-500):].tolist(),band_up,band_mid,band_low],index=['std','upper','mid','lower'],\\\n",
    "                   columns=std_data.index.tolist()[-len(data)+500:]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vaV['ret'] = data.loc[vaV.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vaV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_window = 100\n",
    "\n",
    "var_confidence_level = 0.95\n",
    "test_confidence_level = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_series = []\n",
    "VaR_series = []\n",
    "param_lambda = 0.94\n",
    "param_max_length = 200\n",
    "VaR = None\n",
    "old_VaR = None\n",
    "for i in range(len(vaV)):\n",
    "    sub_vaV = vaV.iloc[:i+1].copy()\n",
    "    if len(sub_vaV) < test_window:\n",
    "        continue\n",
    "    if len(sub_vaV) <= param_max_length:\n",
    "        pass\n",
    "    else:\n",
    "        sub_vaV = sub_vaV.iloc[-param_max_length:]\n",
    "    newest_forecasted_vol = sub_vaV.iloc[-1,1]\n",
    "    sub_vaV['adjusted_ret'] = sub_vaV['ret']*((newest_forecasted_vol/sub_vaV['std']))\n",
    "    index_series.append(sub_vaV.index.tolist()[-1])\n",
    "    bootstrap_factor = 0.6\n",
    "    VaR_sub_list = []\n",
    "    for k in range(10):\n",
    "        selecting = random.sample([j for j in range(len(sub_vaV))],int(len(sub_vaV)*bootstrap_factor))\n",
    "        sub_sub_vaV = sub_vaV.iloc[selecting]\n",
    "        sub_sub_vaV['weights'] = [(param_lambda**(j-1))*(1-param_lambda)/(1-param_lambda**len(sub_sub_vaV)) \\\n",
    "            for j in range(len(sub_sub_vaV))][::-1]\n",
    "        sub_sub_vaV = sub_sub_vaV.sort_values('adjusted_ret')\n",
    "        sub_sub_vaV['weights_cumsum'] = sub_sub_vaV['weights'].cumsum()\n",
    "        VaR = sub_sub_vaV['adjusted_ret'].iloc[next(index for index, value in enumerate(sub_sub_vaV['weights_cumsum']) \\\n",
    "                                                        if value > (1-var_confidence_level))]\n",
    "        VaR_sub_list.append(VaR)\n",
    "    VaR = np.mean(VaR_sub_list)\n",
    "    if old_VaR == None:\n",
    "        old_VaR = VaR\n",
    "        VaR_series.append(VaR)\n",
    "        continue\n",
    "    else:\n",
    "        if VaR > 0.99 * old_VaR:\n",
    "            VaR = 0.99 * old_VaR\n",
    "    old_VaR = VaR\n",
    "    VaR_series.append(VaR)\n",
    "exp_var_up = pd.Series(VaR_series,index=index_series)\n",
    "exp_var_up = pd.ewma(exp_var_up,span = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_series = []\n",
    "VaR_series = []\n",
    "param_lambda = 0.94\n",
    "param_max_length = 200\n",
    "VaR = None\n",
    "old_VaR = None\n",
    "for i in range(len(vaV)):\n",
    "    sub_vaV = vaV.iloc[:i+1].copy()\n",
    "    if len(sub_vaV) < test_window:\n",
    "        continue\n",
    "    if len(sub_vaV) <= param_max_length:\n",
    "        pass\n",
    "    else:\n",
    "        sub_vaV = sub_vaV.iloc[-param_max_length:]\n",
    "    newest_forecasted_vol = sub_vaV.iloc[-1,0]\n",
    "    sub_vaV['adjusted_ret'] = sub_vaV['ret']*((newest_forecasted_vol/sub_vaV['std']))\n",
    "    index_series.append(sub_vaV.index.tolist()[-1])\n",
    "    bootstrap_factor = 0.6\n",
    "    VaR_sub_list = []\n",
    "    for k in range(10):\n",
    "        selecting = random.sample([j for j in range(len(sub_vaV))],int(len(sub_vaV)*bootstrap_factor))\n",
    "        sub_sub_vaV = sub_vaV.iloc[selecting]\n",
    "        sub_sub_vaV['weights'] = [(param_lambda**(j-1))*(1-param_lambda)/(1-param_lambda**len(sub_sub_vaV)) \\\n",
    "            for j in range(len(sub_sub_vaV))][::-1]\n",
    "        sub_sub_vaV = sub_sub_vaV.sort_values('adjusted_ret')\n",
    "        sub_sub_vaV['weights_cumsum'] = sub_sub_vaV['weights'].cumsum()\n",
    "        VaR = sub_sub_vaV['adjusted_ret'].iloc[next(index for index, value in enumerate(sub_sub_vaV['weights_cumsum']) \\\n",
    "                                                        if value > (1-var_confidence_level))]\n",
    "        VaR_sub_list.append(VaR)\n",
    "    VaR = np.mean(VaR_sub_list)\n",
    "    if old_VaR == None:\n",
    "        old_VaR = VaR\n",
    "        VaR_series.append(VaR)\n",
    "        continue\n",
    "    else:\n",
    "        if VaR > 0.99 * old_VaR:\n",
    "            VaR = 0.99 * old_VaR\n",
    "    old_VaR = VaR\n",
    "    VaR_series.append(VaR)\n",
    "exp_var_mid = pd.Series(VaR_series,index=index_series)\n",
    "exp_var_mid = pd.ewma(exp_var_mid,span = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_series = []\n",
    "VaR_series = []\n",
    "param_lambda = 0.94\n",
    "param_max_length = 200\n",
    "VaR = None\n",
    "old_VaR = None\n",
    "for i in range(len(vaV)):\n",
    "    sub_vaV = vaV.iloc[:i+1].copy()\n",
    "    if len(sub_vaV) < test_window:\n",
    "        continue\n",
    "    if len(sub_vaV) <= param_max_length:\n",
    "        pass\n",
    "    else:\n",
    "        sub_vaV = sub_vaV.iloc[-param_max_length:]\n",
    "    newest_forecasted_vol = sub_vaV.iloc[-1,3]\n",
    "    sub_vaV['adjusted_ret'] = sub_vaV['ret']*((newest_forecasted_vol/sub_vaV['std']))\n",
    "    index_series.append(sub_vaV.index.tolist()[-1])\n",
    "    bootstrap_factor = 0.6\n",
    "    VaR_sub_list = []\n",
    "    for k in range(10):\n",
    "        selecting = random.sample([j for j in range(len(sub_vaV))],int(len(sub_vaV)*bootstrap_factor))\n",
    "        sub_sub_vaV = sub_vaV.iloc[selecting]\n",
    "        sub_sub_vaV['weights'] = [(param_lambda**(j-1))*(1-param_lambda)/(1-param_lambda**len(sub_sub_vaV)) \\\n",
    "            for j in range(len(sub_sub_vaV))][::-1]\n",
    "        sub_sub_vaV = sub_sub_vaV.sort_values('adjusted_ret')\n",
    "        sub_sub_vaV['weights_cumsum'] = sub_sub_vaV['weights'].cumsum()\n",
    "        VaR = sub_sub_vaV['adjusted_ret'].iloc[next(index for index, value in enumerate(sub_sub_vaV['weights_cumsum']) \\\n",
    "                                                        if value > (1-var_confidence_level))]\n",
    "        VaR_sub_list.append(VaR)\n",
    "    VaR = np.mean(VaR_sub_list)\n",
    "    if old_VaR == None:\n",
    "        old_VaR = VaR\n",
    "        VaR_series.append(VaR)\n",
    "        continue\n",
    "    else:\n",
    "        if VaR > 0.99 * old_VaR:\n",
    "            VaR = 0.99 * old_VaR\n",
    "    old_VaR = VaR\n",
    "    VaR_series.append(VaR)\n",
    "exp_var_low = pd.Series(VaR_series,index=index_series)\n",
    "exp_var_low = pd.ewma(exp_var_low,span = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "backtest = std_band_backtest_VaR(data, exp_var_mid, exp_var_up, exp_var_low, window = 50, var_cl = 0.95, test_cl = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',20)\n",
    "pd.set_option('display.max_columns',30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_result = []\n",
    "for i in backtest.index:\n",
    "    if backtest.loc[i,'result_VaR_mid'] == 'accept' \\\n",
    "        or backtest.loc[i,'result_VaR_up'] == 'accept' \\\n",
    "        or backtest.loc[i,'result_VaR_low'] == 'accept':\n",
    "        final_result.append('accept')\n",
    "    else:\n",
    "        final_result.append('reject')\n",
    "backtest['final_result'] = final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "backtest['final_result'].value_counts()/len(backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(backtest['VaR_mid'].tolist())\n",
    "ax1.set_ylabel('VaR_mid')\n",
    "\n",
    "ax2 = ax1.twinx()  # this is the important function\n",
    "ax2.plot(backtest['n_exception_VaR_mid'].tolist(),'r.')\n",
    "ax2.hlines(0.5,0,len(backtest))\n",
    "ax2.hlines(6.5,0,len(backtest))\n",
    "ax2.set_ylabel('n_exception_VaR_mid')\n",
    "plt.show()\n",
    "plt.plot((data.loc[backtest.index]/100+1).cumprod().tolist())\n",
    "plt.ylabel('stock')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(backtest['VaR_up'].tolist())\n",
    "ax1.set_ylabel('VaR_up')\n",
    "\n",
    "ax2 = ax1.twinx()  # this is the important function\n",
    "ax2.plot(backtest['n_exception_VaR_up'].tolist(),'r.')\n",
    "ax2.hlines(0.5,0,len(backtest))\n",
    "ax2.hlines(6.5,0,len(backtest))\n",
    "ax2.set_ylabel('n_exception_VaR_up')\n",
    "plt.show()\n",
    "plt.plot((data.loc[backtest.index]/100+1).cumprod().tolist())\n",
    "plt.ylabel('stock')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(backtest['VaR_low'].tolist())\n",
    "ax1.set_ylabel('VaR_low')\n",
    "\n",
    "ax2 = ax1.twinx()  # this is the important function\n",
    "ax2.plot(backtest['n_exception_VaR_low'].tolist(),'r.')\n",
    "ax2.hlines(0.5,0,len(backtest))\n",
    "ax2.hlines(6.5,0,len(backtest))\n",
    "ax2.set_ylabel('n_exception_VaR_low')\n",
    "plt.show()\n",
    "plt.plot((data.loc[backtest.index]/100+1).cumprod().tolist())\n",
    "plt.ylabel('stock')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
